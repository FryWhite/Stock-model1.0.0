# stock_analysis_model_v2.py
# =======================================
# é‡‘èæ¿å—è‚¡ç¥¨æ•°æ®åˆ†ææ¨¡å‹ (å‡çº§ç‰ˆ)
#
# ä¸»è¦åŠŸèƒ½ï¼š
# 1. æ¿å—è”åŠ¨åˆ†æ (åŸºäºä»·æ ¼ç›¸å…³æ€§)
# 2. éšæ€§å­æ¿å—åˆ†ç±» (æ”¯æŒä¸¤ç§æ¨¡å¼åˆ‡æ¢)
#    a. æ¨¡å¼A: åŸºäºå†å²ä»·æ ¼æ”¶ç›Šç‡ (åŸæœ‰é€»è¾‘)
#    b. æ¨¡å¼B: åŸºäºå¤šç»´åº¦æŠ€æœ¯æŒ‡æ ‡ç”»åƒ (æ–°å¢é€»è¾‘)
#
# =======================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import fnmatch
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import warnings

# å¿½ç•¥ä¸€äº›å¸¸è§çš„è­¦å‘Šï¼Œä¿æŒè¾“å‡ºæ•´æ´
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


# === å…³é”®é…ç½®åŒº ===
# ã€è¯·æ ¹æ®ä½ çš„å®é™…æƒ…å†µä¿®æ”¹ä»¥ä¸‹è·¯å¾„å’Œæ¨¡å¼ã€‘

# 1. è®¾ç½®ä½ çš„CSVæ–‡ä»¶æ‰€åœ¨çš„æ–‡ä»¶å¤¹è·¯å¾„
#    ä¾‹å¦‚: 'D:/2508stock/stock/stock data/'
DATA_FOLDER_PATH = 'D:/2508stock/stock/stock data/'

# 2. è®¾ç½®è¾“å‡ºçš„æ–‡ä»¶å¤¹è·¯å¾„
#    ä¾‹å¦‚: 'D:/2508 stock results/'
OUTPUT_FOLDER_PATH = 'D:/2508 stock results/'

# 3. é€‰æ‹©åˆ†æçš„ç›®æ ‡æ¿å—æˆ–ä»£ç æ¨¡å¼
#    å¦‚æœä¸º '', åˆ™åˆ†ææ–‡ä»¶å¤¹é‡Œæ‰€æœ‰CSVæ–‡ä»¶ã€‚
#    ä¾‹å¦‚ï¼š'00000*.SZ' åŒ¹é…æ‰€æœ‰00å¼€å¤´çš„æ·±å¸‚è‚¡ç¥¨
TARGET_STOCK_PATTERN = ''
# === å…³é”®é…ç½®åŒºç»“æŸ ===


# --- è¾…åŠ©å‡½æ•°æ¨¡å— ---

def load_and_prepare_time_series_data(folder_path, pattern=''):
    """
    ã€åŠŸèƒ½ã€‘åŠ è½½å¹¶åˆå¹¶CSVæ•°æ®ï¼Œç”Ÿæˆç”¨äºæ—¶é—´åºåˆ—åˆ†æï¼ˆå¦‚ç›¸å…³æ€§ï¼‰çš„DataFrameã€‚
    ã€è¾“å‡ºã€‘ä¸€ä¸ªé•¿æ ¼å¼çš„DataFrameï¼ŒåŒ…å«æ‰€æœ‰è‚¡ç¥¨çš„å†å²è¡Œæƒ…æ•°æ®ã€‚
    """
    all_data = []
    print(f"\n--- [è¾…åŠ©æ¨¡å—] å¼€å§‹åŠ è½½æ—¶é—´åºåˆ—æ•°æ®: '{folder_path}' ---")
    for filename in os.listdir(folder_path):
        if filename.endswith('.csv'):
            if pattern and not fnmatch.fnmatch(filename, pattern + '*'):
                continue
            
            file_path = os.path.join(folder_path, filename)
            stock_code = filename.split('.')[0]
            print(f" -> æ­£åœ¨åŠ è½½: {filename}")
            try:
                df = pd.read_csv(file_path)
                df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
                df.set_index('trade_date', inplace=True)
                df['ts_code'] = stock_code
                all_data.append(df)
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•åŠ è½½æ–‡ä»¶ {filename}ã€‚é”™è¯¯: {e}")
                
    if not all_data:
        print("é”™è¯¯: åœ¨æŒ‡å®šè·¯å¾„ä¸‹æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„CSVæ–‡ä»¶ï¼")
        return None
        
    combined_df = pd.concat(all_data)
    combined_df.sort_index(inplace=True) # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
    print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…±åŠ è½½ {len(all_data)} åªè‚¡ç¥¨çš„æ•°æ®ã€‚")
    return combined_df

def prepare_clustering_features_from_indicators(folder_path, pattern=''):
    """
    ã€åŠŸèƒ½ã€‘ä»æŠ€æœ¯æŒ‡æ ‡ä¸­æå–æœ€æ–°æ•°æ®ï¼Œæ„å»ºç”¨äºâ€œç”»åƒâ€å¼èšç±»åˆ†æçš„æ¨ªå‘ç‰¹å¾çŸ©é˜µã€‚
    ã€è¾“å‡ºã€‘ä¸€ä¸ªæ ‡å‡†åŒ–çš„DataFrameï¼Œè¡Œæ˜¯è‚¡ç¥¨ä»£ç ï¼Œåˆ—æ˜¯é€‰å®šçš„æŠ€æœ¯æŒ‡æ ‡ã€‚
    """
    print("\n--- [è¾…åŠ©æ¨¡å—] å¼€å§‹å‡†å¤‡æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾çŸ©é˜µ ---")
    
    # ç²¾å¿ƒæŒ‘é€‰çš„6ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼Œåˆ†åˆ«ä»£è¡¨ä»·æ ¼ã€è¶‹åŠ¿ã€åŠ¨èƒ½ã€æ³¢åŠ¨æ€§ã€èµ„é‡‘ã€æ´»è·ƒåº¦
    feature_cols = [
        'pct_chg',                  # æ¶¨è·Œå¹… (çŸ­æœŸä»·æ ¼è¡Œä¸º)
        'macd_dif_bfq',             # MACDå¿«çº¿ (ä¸­æœŸè¶‹åŠ¿æ–¹å‘)
        'rsi_bfq_6',                # 6æ—¥RSI (çŸ­æœŸè¶…ä¹°è¶…å–åŠ¨èƒ½)
        'atr_bfq',                  # ATR (çœŸå®æ³¢åŠ¨å¹…åº¦)
        'mfi_bfq',                  # èµ„é‡‘æµé‡æŒ‡æ ‡ (ä»·é‡é…åˆæƒ…å†µ)
        'turnover_rate'             # æ¢æ‰‹ç‡ (å¸‚åœºæ´»è·ƒåº¦)
    ]
    
    stock_features_dict = {}
    count = 0
    
    for filename in os.listdir(folder_path):
        if filename.endswith('.csv') and (not pattern or fnmatch.fnmatch(filename, pattern + '*')):
            file_path = os.path.join(folder_path, filename)
            stock_code = filename.split('.')[0]
            df = pd.read_csv(file_path)
            df['trade_date'] = pd.to_datetime(df['trade_date'], format='%Y%m%d')
            df.sort_values('trade_date', inplace=True)
            
            # å–æœ€æ–°ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
            latest_data = df.iloc[-1] 
            
            feat_vector = {}
            for col in feature_cols:
                if col in latest_data:
                    feat_vector[col] = latest_data[col]
                else:
                    # å¦‚æœæŸä¸ªæŒ‡æ ‡ç¼ºå¤±ï¼ˆå€¼ä¸ºNaNï¼‰ï¼Œåˆ™ç”¨è¯¥æŒ‡æ ‡åœ¨æ‰€æœ‰è‚¡ç¥¨ä¸­çš„ä¸­ä½æ•°å¡«å……ï¼Œæ¯”å‡å€¼æ›´æŠ—å¹²æ‰°
                    feat_vector[col] = np.nan 
            stock_features_dict[stock_code] = feat_vector
            count += 1
            
    if count == 0:
        print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è‚¡ç¥¨æ•°æ®ï¼")
        return None

    feature_df = pd.DataFrame.from_dict(stock_features_dict, orient='index')
    
    # --- æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ– ---
    # 1. å¡«å……ç¼ºå¤±å€¼ï¼ˆä½¿ç”¨åˆ—çš„ä¸­ä½æ•°ï¼‰
    feature_df = feature_df.fillna(feature_df.median())
    
    # 2. ç‰¹å¾æ ‡å‡†åŒ– (Z-score Normalization)
    # è¿™æ˜¯ä½¿ç”¨ä¸åŒé‡çº²æŒ‡æ ‡è¿›è¡Œèšç±»å‰è‡³å…³é‡è¦çš„ä¸€æ­¥ï¼
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(feature_df)
    
    scaled_feature_df = pd.DataFrame(scaled_features, index=feature_df.index, columns=feature_df.columns)
    
    print(f"æˆåŠŸä» {count} åªè‚¡ç¥¨ä¸­æå–ç‰¹å¾ï¼Œæ„å»ºäº† {scaled_feature_df.shape[0]} x {scaled_feature_df.shape[1]} çš„ç‰¹å¾çŸ©é˜µã€‚")
    return scaled_feature_df


# --- æ ¸å¿ƒåˆ†ææ¨¡å— ---

def perform_correlation_analysis(combined_df, output_path, top_n=10):
    """
    ã€åŠŸèƒ½ã€‘åˆ†ææ¿å—å†…è‚¡ç¥¨çš„ä»·æ ¼è”åŠ¨æ€§ã€‚
    ã€é€»è¾‘ã€‘è®¡ç®—æ‰€æœ‰è‚¡ç¥¨ä¸¤ä¸¤ä¹‹é—´çš„æ”¶ç›Šç‡ç›¸å…³ç³»æ•°ï¼Œç„¶åæ‰¾å‡ºå¹³å‡ç›¸å…³æ€§æœ€é«˜çš„è‚¡ç¥¨ã€‚
    ã€æ³¨æ„ã€‘æ­¤æ¨¡å—ä¾èµ–äº load_and_prepare_time_series_data çš„è¾“å‡ºã€‚
    """
    print("\n--- æ¨¡å—ä¸€ï¼šæ‰§è¡Œæ¿å—è”åŠ¨åˆ†æ ---")
    
    returns_df = combined_df.pivot_table(index=combined_df.index, columns='ts_code', values='close_hfq').pct_change()
    correlation_matrix = returns_df.corr()
    
    # è®¡ç®—æ¯åªè‚¡ç¥¨ä¸å…¶ä»–è‚¡ç¥¨çš„å¹³å‡ç›¸å…³æ€§ï¼ˆæ’é™¤è‡ªç›¸å…³ï¼‰
    avg_correlation = correlation_matrix.mean() - np.diag(correlation_matrix)
    
    top_correlated_stocks = avg_correlation.sort_values(ascending=False).head(top_n)
    
    print(f"æ¿å—å†…å¹³å‡è”åŠ¨æ€§æœ€é«˜çš„ {top_n} åªè‚¡ç¥¨:")
    print(top_correlated_stocks)
    
    plt.figure(figsize=(12, 7))
    top_correlated_stocks.sort_values(ascending=True).plot(kind='barh', color='skyblue')
    plt.title(f'é‡‘èæ¿å—è‚¡ç¥¨å¹³å‡è”åŠ¨æ€§æ’å (Top {top_n})')
    plt.xlabel('å¹³å‡ç›¸å…³ç³»æ•°')
    plt.ylabel('è‚¡ç¥¨ä»£ç ')
    plt.grid(axis='x', linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.savefig(os.path.join(output_path, 'sector_correlation_ranking.png'))
    print(f"è”åŠ¨æ€§åˆ†æç»“æœå·²ä¿å­˜è‡³: {output_path}/sector_correlation_ranking.png")
    plt.close()
    
    return top_correlated_stocks

def perform_cluster_analysis(time_series_data, output_path, n_clusters=3, use_indicator_features=False):
    """
    ã€åŠŸèƒ½ã€‘ä½¿ç”¨èšç±»åˆ†æå¯¹è‚¡ç¥¨è¿›è¡Œéšæ€§å­æ¿å—åˆ†ç±»ã€‚
    ã€é€»è¾‘ã€‘æ ¹æ®å‚æ•°é€‰æ‹©ï¼Œä½¿ç”¨ä¸¤ç§ä¸åŒçš„æ•°æ®æºè¿›è¡Œèšç±»ã€‚
    Args:
        time_series_data (pd.DataFrame): ç”± load_and_prepare_time_series_data äº§ç”Ÿçš„æ—¶é—´åºåˆ—æ•°æ®ã€‚
        output_path (str): è¾“å‡ºè·¯å¾„ã€‚
        n_clusters (int): èšç±»æ•°é‡ã€‚
        use_indicator_features (bool): True=ä½¿ç”¨æ–°æŒ‡æ ‡ï¼ŒFalse=ä½¿ç”¨æ—§ä»·æ ¼æ”¶ç›Šç‡ã€‚
    """
    print("\n--- æ¨¡å—äºŒï¼šæ‰§è¡Œæ¿å—å­åˆ†ç±»åˆ†æ ---")
    print(f"[å½“å‰æ¨¡å¼]: {'ä½¿ç”¨å¤šç»´åº¦æŠ€æœ¯æŒ‡æ ‡' if use_indicator_features else 'ä½¿ç”¨ä»·æ ¼æ”¶ç›Šç‡'}")

    # === æ­¥éª¤1: å‡†å¤‡èšç±»æ•°æ® ===
    if use_indicator_features:
        # åœºæ™¯B: ä½¿ç”¨å¤šç»´åº¦æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        clustering_data = prepare_clustering_features_from_indicators(DATA_FOLDER_PATH, TARGET_STOCK_PATTERN)
        if clustering_data is None:
            print("é”™è¯¯: è·å–æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾å¤±è´¥ï¼Œæ— æ³•æ‰§è¡Œèšç±»ã€‚")
            return None
        # æ•°æ®æœ¬èº«å°±æ˜¯æ ‡å‡†åŒ–çš„ï¼Œä¸”è¡Œç´¢å¼•æ˜¯è‚¡ç¥¨ä»£ç 
        scaled_data = clustering_data.values
        stock_codes = clustering_data.index
        description_suffix = " (åŸºäºæŠ€æœ¯æŒ‡æ ‡ç”»åƒ)"
    else:
        # åœºæ™¯A: ä½¿ç”¨ä»·æ ¼æ”¶ç›Šç‡
        if time_series_data is None:
            print("é”™è¯¯: ç¼ºå°‘æ—¶é—´åºåˆ—æ•°æ®ï¼Œæ— æ³•æ‰§è¡Œæ­¤æ¨¡å¼èšç±»ã€‚")
            return None
            
        returns_df = time_series_data.pivot_table(index=time_series_data.index, columns='ts_code', values='close_hfq').dropna()
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(returns_df.T) # å¯¹è‚¡ç¥¨è¿›è¡Œèšç±»
        stock_codes = returns_df.columns
        description_suffix = " (åŸºäºä»·æ ¼è”åŠ¨)"

    # === æ­¥éª¤2: ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™è¾…åŠ©é€‰æ‹©Kå€¼ (å¯è§†åŒ–) ===
    print("æ­£åœ¨ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™å¯»æ‰¾æœ€ä½³èšç±»æ•°...")
    inertias = []
    max_k = min(len(stock_codes), 10) 
    K_range = range(1, max_k)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(scaled_data)
        inertias.append(kmeans.inertia_)
        
    plt.figure(figsize=(10, 5))
    plt.plot(K_range, inertias, 'bo-')
    plt.xlabel('Number of clusters (k)')
    plt.ylabel('Inertia')
    plt.title('Elbow Method For Optimal k' + description_suffix)
    plt.grid(True)
    plt.savefig(os.path.join(output_path, f'elbow_method_for_k{"_indicator" if use_indicator_features else "_returns"}.png'))
    print(f"è‚˜éƒ¨æ³•åˆ™å›¾å·²ä¿å­˜è‡³: {output_path}/elbow_method_for_k...")
    plt.close()
    
    # === æ­¥éª¤3: æ‰§è¡Œæœ€ç»ˆèšç±» ===
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(scaled_data)
    
    cluster_results = pd.Series(cluster_labels, index=stock_codes, name='cluster')
    
    print(f"å·²å®Œæˆ {n_clusters} ä¸ªèšç±»çš„åˆ’åˆ†ã€‚")
    print("å„èšç±»åŒ…å«çš„è‚¡ç¥¨:")
    for i in range(n_clusters):
        stocks_in_cluster = cluster_results[cluster_results == i].index.tolist()
        print(f"  - Cluster {i}: {len(stocks_in_cluster)} åªè‚¡ç¥¨")
        print(f"    {', '.join(stocks_in_cluster[:5])}...")
        
    # === æ­¥éª¤4: å¯è§†åŒ–èšç±»ç»“æœ ===
    # ä¸ºæ¯ä¸ªèšç±»ç”Ÿæˆä¸€ä¸ªè¡¨æ ¼ï¼Œå±•ç¤ºå…¶ç‰¹å¾å‡å€¼ï¼Œç”¨äºè§£è¯»èšç±»æ€§è´¨
    print("\n--- å„èšç±»ç‰¹å¾å‡å€¼è§£è¯» ---")
    if use_indicator_features:
        # å¦‚æœä½¿ç”¨æŒ‡æ ‡èšç±»ï¼Œå±•ç¤ºç‰¹å¾å‡å€¼æ¥è§£é‡Šæ¯ä¸ªèšç±»çš„â€œç”»åƒâ€
        cluster_means = cluster_results.to_frame().join(clustering_data).groupby('cluster').mean()
        print(cluster_means.round(2)) # ä¿ç•™ä¸¤ä½å°æ•°ï¼Œæ–¹ä¾¿é˜…è¯»

    # å¯è§†åŒ–ä»£è¡¨è‚¡ç¥¨çš„èµ°åŠ¿å›¾ (ä»…é€‚ç”¨äºä»·æ ¼æ”¶ç›Šç‡æ¨¡å¼)
    if not use_indicator_features and time_series_data is not None:
        print("\næ­£åœ¨ç»˜åˆ¶å„èšç±»æ ·æœ¬è‚¡ç¥¨èµ°åŠ¿å›¾...")
        for i in range(n_clusters):
            cluster_stocks = cluster_results[cluster_results == i].index
            if len(cluster_stocks) > 0:
                sample_stock = cluster_stocks[0]
                plt.figure(figsize=(12, 6))
                time_series_data[time_series_data['ts_code'] == sample_stock]['close_qfq'].plot(title=f'Cluster {i} Sample Stock: {sample_stock}')
                plt.ylabel('å‰å¤æƒæ”¶ç›˜ä»·')
                plt.grid(True)
                plt.savefig(os.path.join(output_path, f'cluster_{i}_sample.png'))
                plt.close()
    elif use_indicator_features:
        print("(æŒ‡æ ‡èšç±»æ¨¡å¼) ç‰¹å¾å‡å€¼è¡¨æ ¼å·²åœ¨ä¸Šæ–¹è¾“å‡ºï¼Œå¯æ®æ­¤è§£è¯»å„èšç±»æ€§è´¨ã€‚")
        # è¿›é˜¶ï¼šå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ç”»é›·è¾¾å›¾çš„ä»£ç ï¼Œä½¿è§£è¯»æ›´ç›´è§‚ã€‚
    
    return cluster_results


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # åˆå§‹åŒ–å®Œæ•´è·¯å¾„
    data_dir = os.path.normpath(DATA_FOLDER_PATH)
    output_dir = os.path.normpath(OUTPUT_FOLDER_PATH)

    # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"å·²åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: {output_dir}")

    # --- é€‰æ‹©åˆ†ææ–¹æ³• ---
    # åœ¨è¿™é‡Œè®¾ç½® True æˆ– Falseï¼Œæ¥åˆ‡æ¢èšç±»åˆ†æçš„æ¨¡å¼ï¼
    # True -> æ–°æ–¹æ³•ï¼šå¤šç»´åº¦æŠ€æœ¯æŒ‡æ ‡ç”»åƒ
    # False -> æ—§æ–¹æ³•ï¼šä»·æ ¼æ”¶ç›Šç‡è”åŠ¨
    USE_INDICATOR_CLUSTERING = True

    # æ‰§è¡Œæ¨¡å—ä¸€ï¼šæ¿å—è”åŠ¨åˆ†æ (æ­¤æ¨¡å—ä¸ä¾èµ–èšç±»æ¨¡å¼ï¼Œå§‹ç»ˆä½¿ç”¨ä»·æ ¼æ•°æ®)
    full_time_series_data = load_and_prepare_time_series_data(data_dir, TARGET_STOCK_PATTERN)
    
    if full_time_series_data is not None:
        correlation_results = perform_correlation_analysis(full_time_series_data, output_dir)
        
        # æ‰§è¡Œæ¨¡å—äºŒï¼šæ¿å—å­åˆ†ç±» (æ ¹æ®ä½ çš„é€‰æ‹©æ¥æ‰§è¡Œ)
        print("\n" + "="*50)
        if USE_INDICATOR_CLUSTERING:
            print("!!! å½“å‰è¿è¡Œæ¨¡å¼ï¼šå¤šç»´åº¦æŠ€æœ¯æŒ‡æ ‡èšç±» !!!")
        else:
            print("!!! å½“å‰è¿è¡Œæ¨¡å¼ï¼šä»·æ ¼æ”¶ç›Šç‡èšç±» !!!")
        print("="*50)
            
        cluster_results = perform_cluster_analysis(
            full_time_series_data, 
            output_dir, 
            n_clusters=3, # å¯ä»¥ä¿®æ”¹è¿™ä¸ªæ•°å­—æ¥æ”¹å˜èšç±»æ•°é‡
            use_indicator_features=USE_INDICATOR_CLUSTERING
        )

        # ä¿å­˜åˆ†æç»“æœåˆ°CSV
        if cluster_results is not None and correlation_results is not None:
            correlation_results.to_csv(os.path.join(output_dir, 'top_correlated_stocks.csv'))
            cluster_results.to_csv(os.path.join(output_dir, f'stock_cluster_assignments{"_indicator" if USE_INDICATOR_CLUSTERING else "_returns"}.csv'))
            
            print("\n" + "="*50)
            print("ğŸ‰ æ‰€æœ‰åˆ†ææ¨¡å—è¿è¡Œå®Œæˆï¼")
            print("="*50)
            print(f"è¯·æ£€æŸ¥è¾“å‡ºæ–‡ä»¶å¤¹: {output_dir}")
            print("ç”Ÿæˆçš„æ–‡ä»¶åŒ…æ‹¬ï¼š")
            print("  - sector_correlation_ranking.png (æ¿å—è”åŠ¨æ’åå›¾)")
            print("  - elbow_method_for_k...png (èšç±»è‚˜éƒ¨æ³•åˆ™å›¾)")
            if not USE_INDICATOR_CLUSTERING:
                print("  - cluster_0_sample.png ... (å„åˆ†ç±»æ ·æœ¬èµ°åŠ¿å›¾)")
            print("  - top_correlated_stocks.csv (è”åŠ¨æ€§æ’åæ•°æ®)")
            print("  - stock_cluster_assignments...csv (è‚¡ç¥¨åˆ†ç±»ç»“æœï¼Œæ–‡ä»¶åå«æ¨¡å¼æ ‡è¯†)")
        else:
            print("åˆ†æè¿‡ç¨‹ä¸­æ–­ï¼Œæœªç”Ÿæˆæœ€ç»ˆç»“æœã€‚")
